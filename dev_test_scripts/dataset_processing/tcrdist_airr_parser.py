#!/usr/bin/env python3
"""
AIRR per-repertoire TSVs -> tcrdist loops for beta chain.

Input:
  - processed/<dataset>/export/<repertoire_id>.tsv (generated by airr_split_rearrangements.py)
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import glob
import ctypes
import sys
from pathlib import Path
from typing import Dict, List

import pandas as pd
from tqdm.auto import tqdm

from tcrtyper.config import config
from tcrtyper.dataset_processing.path_utils import processed_dataset_root
from tcrtyper.dataset_processing.airr_split_rearrangements import (
    find_single_tsv,
    split_rearrangement_tsv,
)
from tcrtyper.dataset_processing.tcrdist_loops_core import run_tcrdist_loops

logger = logging.getLogger(__name__)


# FIXME preload conda's libstdc++.so.6
def _preload_conda_libstdcxx():
    try:
        conda_prefix = os.environ.get("CONDA_PREFIX", "").strip()
        conda_lib = os.path.join(conda_prefix, "lib") if conda_prefix else ""
        if conda_lib and os.path.isdir(conda_lib):
            candidates = (
                glob.glob(os.path.join(conda_lib, "libstdc++.so.6*"))
                + glob.glob(os.path.join(conda_lib, "libstdc++.so*"))
            )
            for p in candidates:
                try:
                    ctypes.CDLL(p, mode=ctypes.RTLD_GLOBAL)
                    os.environ["LD_LIBRARY_PATH"] = (
                        f"{conda_lib}:{os.environ.get('LD_LIBRARY_PATH', '')}"
                    )
                    break
                except OSError:
                    continue
    except Exception:
        pass


_preload_conda_libstdcxx()

def discover_export_files(export_dir: Path) -> List[Path]:
    if not export_dir.is_dir():
        return []
    return sorted(p for p in export_dir.glob("*.tsv") if p.is_file())


def run_one_sample(
    export_tsv: Path,
    processed_dir: Path,
    tcrdist_debug: bool = False,
) -> Dict[str, int | str]:
    sample_name = export_tsv.stem
    df_raw = pd.read_csv(export_tsv, sep="\t", dtype=str)

    required = ["cdr3_aa", "v_call", "j_call", "count"]
    missing = [c for c in required if c not in df_raw.columns]
    if missing:
        raise ValueError(f"{export_tsv}: missing required columns: {missing}")

    cdr3 = df_raw["cdr3_aa"].astype(str)
    v_gene = df_raw["v_call"].astype(str)
    j_gene = df_raw["j_call"].astype(str)
    count = pd.to_numeric(df_raw["count"], errors="coerce").fillna(1).astype(int)

    mask = (
        cdr3.notna()
        & (cdr3.astype(str) != "")
        & v_gene.notna()
        & (v_gene.astype(str) != "")
        & j_gene.notna()
        & (j_gene.astype(str) != "")
    )

    n_raw = int(len(df_raw))
    n_after_basic_filter = int(mask.sum())

    cell_df = pd.DataFrame(
        {
            "cdr3_b_aa": cdr3[mask].astype(str),
            "v_b_gene": v_gene[mask].astype(str),
            "j_b_gene": j_gene[mask].astype(str),
            "count": count[mask].astype(int),
        }
    )

    loops_df = run_tcrdist_loops(
        cell_df,
        organism="human",
        chain="beta",
        debug=tcrdist_debug,
    )

    processed_dir.mkdir(parents=True, exist_ok=True)
    processed_out = processed_dir / f"{sample_name}.tsv"
    loops_df.to_csv(processed_out, sep="\t", index=False)

    return {
        "input": str(export_tsv),
        "processed_out": str(processed_out),
        "n_raw": n_raw,
        "n_after_basic_filter": n_after_basic_filter,
        "n_with_loops": int(len(loops_df)),
    }


def _run_one_sample_star(args):
    return run_one_sample(*args)


def _resolve_log_every(log_every: int, n_cores: int, show_progress: bool) -> int:
    if log_every is None:
        log_every = 0
    if log_every > 0:
        return log_every
    if n_cores and n_cores > 1:
        return 200
    if not show_progress:
        return 200
    if not sys.stderr.isatty():
        return 200
    return 0


def process_single_dataset(
    dataset_dir: str,
    tcrdist_debug: bool,
    show_progress: bool,
    n_cores: int,
    one_per_dataset: bool,
    log_every: int,
    split_if_needed: bool,
    rearrangement_tsv: str | None,
    count_field: str | None,
    max_open_files: int,
) -> dict:
    dataset_path = Path(dataset_dir)
    dataset_name = dataset_path.name
    out_root = processed_dataset_root(dataset_path)
    processed_dir = out_root / config.data.tcrdist_processed_subdir_name
    processed_dir.mkdir(parents=True, exist_ok=True)

    export_dir = processed_dataset_root(dataset_path) / config.data.export_subdir_name
    split_report = None

    if split_if_needed and not discover_export_files(export_dir):
        src = (
            dataset_path / rearrangement_tsv
            if rearrangement_tsv
            else find_single_tsv(dataset_path)
        )
        if not src.exists():
            raise FileNotFoundError(f"Rearrangement TSV not found: {src}")
        logger.info("[%s] splitting %s -> %s", dataset_name, src.name, export_dir)
        split_report = split_rearrangement_tsv(
            src,
            export_dir,
            repertoire_id_field="repertoire_id",
            locus_field="locus",
            cdr3_field="cdr3_aa",
            v_field="v_call",
            j_field="j_call",
            count_field=count_field,
            max_open_files=max_open_files,
        )
        if split_report["counts"]["rows_non_trb"]:
            logger.warning(
                "[%s] dropped %d rows with non-TRB locus.",
                dataset_name,
                split_report["counts"]["rows_non_trb"],
            )
        if split_report["counts"]["rows_missing_fields"]:
            logger.warning(
                "[%s] dropped %d rows with missing required fields.",
                dataset_name,
                split_report["counts"]["rows_missing_fields"],
            )
    files = discover_export_files(export_dir)
    reports: list[dict] = []

    if not files:
        logger.warning(
            "No per-repertoire TSVs found in %s (run airr_split_rearrangements.py first).",
            export_dir,
        )
        return {dataset_name: {"processed": reports, "split": split_report}}

    if one_per_dataset:
        rep = run_one_sample(files[0], processed_dir, tcrdist_debug=tcrdist_debug)
        reports.append(rep)
        return {dataset_name: {"processed": reports, "split": split_report}}

    tasks = [(f, processed_dir, tcrdist_debug) for f in files]
    total = len(tasks)
    log_every = _resolve_log_every(log_every, n_cores, show_progress)
    logger.info("[%s] starting %d repertoire(s) (n_cores=%s)", dataset_name, total, n_cores)
    if log_every:
        logger.info("[%s] progress 0/%d", dataset_name, total)

    if n_cores is None or n_cores <= 1 or len(tasks) == 1:
        iterable = tqdm(
            tasks,
            desc=dataset_name,
            unit="file",
            disable=not show_progress,
            file=sys.stderr,
            mininterval=10,
            leave=True,
        )
        for idx, (f, pdir, dbg) in enumerate(iterable, 1):
            rep = run_one_sample(f, pdir, tcrdist_debug=dbg)
            reports.append(rep)
            if log_every and (idx % log_every == 0 or idx == total):
                logger.info("[%s] progress %d/%d", dataset_name, idx, total)
    else:
        from multiprocessing import Pool

        iterable = tqdm(
            Pool(processes=n_cores).imap_unordered(_run_one_sample_star, tasks),
            total=len(tasks),
            desc=dataset_name,
            unit="file",
            disable=not show_progress,
            file=sys.stderr,
            mininterval=10,
            leave=True,
        )
        for idx, rep in enumerate(iterable, 1):
            reports.append(rep)
            if log_every and (idx % log_every == 0 or idx == total):
                logger.info("[%s] progress %d/%d", dataset_name, idx, total)

    return {dataset_name: {"processed": reports, "split": split_report}}


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        description=(
            "Process AIRR rearrangement TSVs (beta chain) and infer CDR loops."
        )
    )
    p.add_argument(
        "--dataset-dir",
        required=True,
        help="Path to a single AIRR dataset directory.",
    )
    p.add_argument(
        "--debug",
        action="store_true",
        default=False,
        help="Enable debug logging and verbose tcrdist internals.",
    )
    p.add_argument(
        "--split-if-needed",
        action="store_true",
        default=False,
        help="If processed export/ is empty, split the rearrangement TSV first.",
    )
    p.add_argument(
        "--rearrangement-tsv",
        default=None,
        help="Rearrangement TSV filename under dataset dir (default: auto-detect single *.tsv).",
    )
    p.add_argument(
        "--count-field",
        default=None,
        help="Override count field used when splitting (default: auto-pick).",
    )
    p.add_argument(
        "--max-open-files",
        type=int,
        default=128,
        help="Maximum number of per-repertoire files kept open while splitting.",
    )
    p.add_argument(
        "--one-per-dataset",
        action="store_true",
        default=False,
        help="Process only the first repertoire (debug / quick run).",
    )
    p.add_argument(
        "--no-progress",
        action="store_true",
        default=False,
        help="Disable tqdm progress bars.",
    )
    p.add_argument(
        "--summary",
        default=None,
        help=(
            "Path to write run summary JSON "
            f"(default: processed/<dataset>/{config.data.tcrdist_summary_filename})"
        ),
    )
    p.add_argument(
        "--n-cores",
        type=int,
        default=8,
        help="Number of parallel worker processes per dataset (default: 8).",
    )
    p.add_argument(
        "--log-every",
        type=int,
        default=0,
        help=(
            "Log progress every N samples (0 disables). "
            "If 0, defaults to 200 for non-TTY, --no-progress, or n-cores>1."
        ),
    )
    return p.parse_args()


def _configure_logging(debug: bool) -> None:
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def main() -> None:
    args = parse_args()
    _configure_logging(args.debug)

    reports = process_single_dataset(
        dataset_dir=args.dataset_dir,
        tcrdist_debug=args.debug,
        show_progress=not args.no_progress,
        n_cores=args.n_cores,
        one_per_dataset=args.one_per_dataset,
        log_every=args.log_every,
        split_if_needed=args.split_if_needed,
        rearrangement_tsv=args.rearrangement_tsv,
        count_field=args.count_field,
        max_open_files=args.max_open_files,
    )

    dataset_path = Path(args.dataset_dir)
    out_root = processed_dataset_root(dataset_path)
    out_root.mkdir(parents=True, exist_ok=True)
    summary_default = out_root / config.data.tcrdist_summary_filename
    summary_path = Path(args.summary) if args.summary is not None else summary_default
    summary_path.parent.mkdir(parents=True, exist_ok=True)

    with open(summary_path, "w", encoding="utf-8") as fh:
        json.dump(reports, fh, indent=2, ensure_ascii=False)

    for ds, rep in reports.items():
        processed = rep.get("processed", [])
        logger.info(
            "[%s] processed %d repertoire(s); one_per_dataset=%s",
            ds,
            len(processed),
            args.one_per_dataset,
        )


if __name__ == "__main__":
    main()
